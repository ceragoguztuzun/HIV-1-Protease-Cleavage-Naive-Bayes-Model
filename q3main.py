# -*- coding: utf-8 -*-
"""ML HW1

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ok1yKflZzV_1r36h29sdJwgEsypxIXVC

Q3.1 - Training a Bernoulli Naive Bayes model
"""

import numpy  as np 
import pandas as pd
import sys
import matplotlib.pyplot as plt

cleavePos = []
cleaveNeg = []

# load train data
with open('q2_train_set.txt') as f:
    seqData_train = f.readlines()
    for eachSeqData_train in seqData_train:
        seqDataArr = np.fromstring(eachSeqData_train, dtype=int, sep=',')
        N_features = seqDataArr.shape[0] - 1
        # clasify test data as cleave positive and cleave negative
        if (seqDataArr[-1] == 1):
            cleavePos.append(seqDataArr)
        elif (seqDataArr[-1] == 0):
            cleaveNeg.append(seqDataArr)

cleavePos = np.asarray(cleavePos)
cleaveNeg = np.asarray(cleaveNeg)

# calculating T(j,y=0)
t_jy0 = cleaveNeg.sum(axis = 0)
# calculating feature count
featureCount = cleavePos.shape[1]
# calculating T(j,y=1)
t_jy1 = cleavePos.sum(axis = 0)
# calculating N0
n0 = cleaveNeg.shape[0]
# calculating N1
n1 = cleavePos.shape[0]
# calculating N
n = n1 + n0

# calculating P(Xj|Y=0)
prob_xj_y0 = t_jy0 / t_jy0.sum()
# calculating P(Xj|Y=1)
prob_xj_y1 = t_jy1 / t_jy1.sum()
# calculating P(Y=0)
prob_cleaveNeg = n0 / n
# calculating P(Y=1)
prob_cleavePos = n1 / n

prob_xj_y1 = prob_xj_y1.astype(float)
prob_xj_y0 = prob_xj_y0.astype(float)

testData = []
testData_labelResults = []
# loading test data
with open('q2_test_set.txt') as f:
  seqData_test = f.readlines()
  for eachSeqData_test in seqData_test:
     testDataArr = np.fromstring(eachSeqData_test, dtype=int, sep=',')
     testData.append(testDataArr)
     testData_labelResults.append(testDataArr[-1])
testData = np.asarray(testData)
testData_labelResults = np.asarray(testData_labelResults)
N_train = testData.shape[0]

# obtain estimate (yi) for each test data
cleavePos_yi_max = np.zeros(N_train)
cleaveNeg_yi_max = np.zeros(N_train)
prob_x1_y0 = np.zeros(N_features)
prob_x1_y1 = np.zeros(N_features)
for j in range(N_features):
    prob_x1_y0[j] = np.sum(cleaveNeg, axis=0)[j] / cleaveNeg.shape[0]
    prob_x1_y1[j] = np.sum(cleavePos, axis=0)[j] / cleavePos.shape[0]
N_features = featureCount-1
for i in range(N_train):
  cleavePos_yi = np.zeros( N_features)
  cleaveNeg_yi = np.zeros( N_features)
    
  for j in range(N_features):
    if (testData[:,j][i] == 1):
      cleaveNeg_yi[j] = float(np.log(prob_x1_y0[j]))
      cleavePos_yi[j] = float(np.log(prob_x1_y1[j]))
    else:
      cleaveNeg_yi[j] = float(np.log(1-prob_xj_y0[j]))
      cleavePos_yi[j] = float(np.log(1-prob_xj_y1[j]))
  cleavePos_yi_max[i] = (np.log(prob_cleavePos) + float(np.sum(cleavePos_yi)))
  cleaveNeg_yi_max[i] = (np.log(prob_cleaveNeg) + float(np.sum(cleaveNeg_yi)))
predicted_labels = []
for i in range(N_train):
  if (cleavePos_yi_max[i] > cleaveNeg_yi_max[i]):
    predicted_labels.append(1)
  else: 
    predicted_labels.append(0)
predicted_labels = np.array(predicted_labels)

# compare predicted labels with test data's labels!
# calculate Accuracy
match_count = 0
fail_count = 0
for i in range(N_train):
  if (testData_labelResults[i] == predicted_labels[i]):
    match_count += 1
  else:
    fail_count += 1
accuracy = 100 * (match_count / (fail_count + match_count))
#output
print("MATCH: ",match_count)
print("FAIL: ",fail_count)
print("ACCURACY: ",accuracy)

"""Q3.2 Finding cleavage of gag polyprotein"""

# encoding
g_enc = "10000000000000000000"
p_enc = "01000000000000000000"
a_enc = "00100000000000000000"
v_enc = "00010000000000000000"
l_enc = "00001000000000000000"
i_enc = "00000100000000000000"
m_enc = "00000010000000000000"
c_enc = "00000001000000000000"
f_enc = "00000000100000000000"
y_enc = "00000000010000000000"
w_enc = "00000000001000000000"
h_enc = "00000000000100000000"
k_enc = "00000000000010000000"
r_enc = "00000000000001000000"
q_enc = "00000000000000100000"
n_enc = "00000000000000010000"
e_enc = "00000000000000001000"
d_enc = "00000000000000000100"
s_enc = "00000000000000000010"
t_enc = "00000000000000000001"

# load gag sequence data
with open('q2_gag_sequence.txt') as f:
    gagSeq = f.readlines()
gagSeq = list(gagSeq[0])
gagSeq = np.asarray(gagSeq)

# create encoding
def encode(aList):
  temp_arr = []
  encoded_arr = []
  i = 0
  j = 0
  while len(encoded_arr) != len(aList)-7:

    if (aList[i] == 'g'):
      temp_arr.append(g_enc)
    elif (aList[i] == 'p'):
      temp_arr.append(p_enc)
    elif (aList[i] == 'a'):
      temp_arr.append(a_enc)
    elif (aList[i] == 'v'):
      temp_arr.append(v_enc)
    elif (aList[i] == 'l'):
      temp_arr.append(l_enc)
    elif (aList[i] == 'i'):
      temp_arr.append(i_enc)
    elif (aList[i] == 'm'):
      temp_arr.append(m_enc)
    elif (aList[i] == 'c'):
      temp_arr.append(c_enc)
    elif (aList[i] == 'f'):
      temp_arr.append(f_enc)
    elif (aList[i] == 'y'):
      temp_arr.append(y_enc)
    elif (aList[i] == 'w'):
      temp_arr.append(w_enc)
    elif (aList[i] == 'h'):
      temp_arr.append(h_enc)
    elif (aList[i] == 'k'):
      temp_arr.append(k_enc)
    elif (aList[i] == 'r'):
      temp_arr.append(r_enc)
    elif (aList[i] == 'q'):
      temp_arr.append(q_enc)
    elif (aList[i] == 'n'):
      temp_arr.append(n_enc)
    elif (aList[i] == 'e'):
      temp_arr.append(e_enc)
    elif (aList[i] == 'd'):
      temp_arr.append(d_enc)
    elif (aList[i] == 's'):
      temp_arr.append(s_enc)
    elif (aList[i] == 't'):
      temp_arr.append(t_enc)

    i += 1
    j += 1
    if j % 8 == 0:
      encoded_8mer = "".join(temp_arr)
      encoded_8mer = list(map(int, encoded_8mer))
      encoded_arr.append(encoded_8mer)
      temp_arr = []
      encoded_8mer = ""
      i = len(encoded_arr)

  encoded_arr = np.array(encoded_arr)
  return encoded_arr
encoded_arr = np.array(encode(gagSeq))

# obtain estimate (yi) for each test data
N_features = encoded_arr.shape[1]
N_test = encoded_arr.shape[0]

cleavePos_yi_max = np.zeros(N_test)
cleaveNeg_yi_max = np.zeros(N_test)
prob_x1_y0 = np.zeros(N_features)
prob_x1_y1 = np.zeros(N_features)

for j in range(N_features):
    prob_x1_y0[j] = np.sum(cleaveNeg, axis=0)[j] / cleaveNeg.shape[0]
    prob_x1_y1[j] = np.sum(cleavePos, axis=0)[j] / cleavePos.shape[0]

for i in range(N_test):
  cleavePos_yi = np.zeros( N_features)
  cleaveNeg_yi = np.zeros( N_features)
  
  for j in range(N_features):
     if (encoded_arr[:,j][i] == 1):
       cleaveNeg_yi[j] = np.log(prob_x1_y0[j])
       cleavePos_yi[j] = np.log(prob_x1_y1[j])
     else:
       cleaveNeg_yi[j] = np.log(1-prob_xj_y0[j])
       cleavePos_yi[j] = np.log(1-prob_xj_y1[j])
    
  cleavePos_yi_max[i] = (np.log(prob_cleavePos) + np.sum(cleavePos_yi))
  cleaveNeg_yi_max[i] = (np.log(prob_cleaveNeg) + np.sum(cleaveNeg_yi))
  
predicted_labels = []
for i in range(N_test):
  if (cleavePos_yi_max[i] > cleaveNeg_yi_max[i]):
    predicted_labels.append(1)
  else: 
    predicted_labels.append(0)
predicted_labels = np.array(predicted_labels)

# reverse encoding
def reverseEncode(aList):
  temp_arr = []
  encoded_arr = []
  i = 0
  j = 0
  while len(temp_arr) != 8:

    if (aList[i] == 1):
      temp_arr.append('G')
    elif (aList[i+1] == 1):
      temp_arr.append('P')
    elif (aList[i+2] == 1):
      temp_arr.append('A')
    elif (aList[i+3] == 1):
      temp_arr.append('V')
    elif (aList[i+4] == 1):
      temp_arr.append('L')
    elif (aList[i+5] == 1):
      temp_arr.append('I')
    elif (aList[i+6] == 1):
      temp_arr.append('M')
    elif (aList[i+7] == 1):
      temp_arr.append('C')
    elif (aList[i+8] == 1):
      temp_arr.append('F')
    elif (aList[i+9] == 1):
      temp_arr.append('Y')
    elif (aList[i+10] == 1):
      temp_arr.append('W')
    elif (aList[i+11] == 1):
      temp_arr.append('H')
    elif (aList[i+12] == 1):
      temp_arr.append('K')
    elif (aList[i+13] == 1):
      temp_arr.append('R')
    elif (aList[i+14] == 1):
      temp_arr.append('Q')
    elif (aList[i+15] == 1):
      temp_arr.append('N')
    elif (aList[i+16] == 1):
      temp_arr.append('E')
    elif (aList[i+17] == 1):
      temp_arr.append('D')
    elif (aList[i+18] == 1):
      temp_arr.append('S')
    elif (aList[i+19] == 1):
      temp_arr.append('T')

    i += 1
    
  encoded_str = ""
  for j in temp_arr: 
      encoded_str += j  
  return encoded_str

# finding clevage indices
for i in range(predicted_labels.size):
  if predicted_labels[i] == 1:
    pos = (i+1) * 8 - 4 
    print(i+1,"th 8mer -> pos:", pos, "-",pos+1)
minConfident_index = np.where(cleaveNeg_yi_max == np.amin(cleaveNeg_yi_max))
maxConfident_index = np.where(cleavePos_yi_max == np.amax(cleavePos_yi_max))

print("most confident 8mer with positive cleavage: ", reverseEncode(encoded_arr[maxConfident_index[0][0]]),
      " at index: ",maxConfident_index[0][0]+1, " with confidence: " ,cleavePos_yi_max[maxConfident_index][0],
      )
print("least confident 8mer with negative cleavage: ", reverseEncode(encoded_arr[minConfident_index[0][0]]),
      " at index: ",minConfident_index[0][0]+1, " with confidence: ",cleaveNeg_yi_max[minConfident_index][0])

"""DISCUSSION: 
most confident 8mer with positive cleavage:  APGTSDEN  at index:  360  with confidence:  -17.313659730372596
least confident 8mer with negative cleavage:  RKHWYFCM  at index:  409  with confidence:  -29.967623100215004
The sequences vary, which shows that the confidence of cleavage for each sequence varies mostly due to the sequence of the amino acids.

Q3.4 Estending the classifier to compute MAP Estimates pf parameters using a fair Dirichlet prior.
"""

def getEstimates(alpha):
  N_features = testData.shape[1]
  N_train = testData.shape[0]
  cleavePos_yi_max = np.zeros(N_train)
  cleaveNeg_yi_max = np.zeros(N_train)
  prob_x1_y0 = np.zeros(N_features)
  prob_x1_y1 = np.zeros(N_features)
  for j in range(N_features):
    prob_x1_y0[j] = (np.sum(cleaveNeg, axis=0)[j] + alpha) / (2*alpha + cleaveNeg.shape[0])
    prob_x1_y1[j] = (np.sum(cleavePos, axis=0)[j] + alpha) / (2*alpha + cleavePos.shape[0])

  for i in range(N_train):
    cleavePos_yi = np.zeros( N_features)
    cleaveNeg_yi = np.zeros( N_features)
  
    for j in range(N_features):
     if (testData[:,j][i] == 1):
       cleaveNeg_yi[j] = np.log(prob_x1_y0[j])
       cleavePos_yi[j] = np.log(prob_x1_y1[j])
     else:
        cleaveNeg_yi[j] = np.log(1-prob_xj_y0[j])
        cleavePos_yi[j] = np.log(1-prob_xj_y1[j])
    
    cleavePos_yi_max[i] = (np.log(prob_cleavePos) + np.sum(cleavePos_yi))
    cleaveNeg_yi_max[i] = (np.log(prob_cleaveNeg) + np.sum(cleaveNeg_yi))
  predicted_labels = []
  for i in range(N_train):
    if (cleavePos_yi_max[i] > cleaveNeg_yi_max[i]):
      predicted_labels.append(1)
    else: 
      predicted_labels.append(0)
  predicted_labels = np.array(predicted_labels)

  # calculating accuracy
  match_count = 0
  fail_count = 0
  for i in range(N_train):
   if (testData_labelResults[i] == predicted_labels[i]):
      match_count += 1
   else:
     fail_count += 1
  accuracy = 100 * (match_count / (fail_count + match_count))
  return accuracy

# alpha = 0 is already obtained.
# alpha = {0,1,2,3,4,5,6,7,8,9,10}
accuracyValues = []
for alpha in range(11):
  accuracyValues.append(getEstimates(alpha))

# Plot alpha vs. test set accuracy
plt.plot([i for i in range(0, 11)],accuracyValues)
plt.xlabel('Alpha Values')
plt.ylabel('Accuracy Values')
plt.title("Alpha vs. Test Set Accuracy")
plt.show()

#obtain the first 50 rows of training set
cleavePos = []
cleaveNeg = []
counter = 0
# load train data
with open('q2_train_set.txt') as f:
    seqData_train = f.readlines()
    for eachSeqData_train in seqData_train:
      counter += 1
      if counter < 51:
        seqDataArr = np.fromstring(eachSeqData_train, dtype=int, sep=',')
        N_features = seqDataArr.shape[0] - 1
        # clasify test data as cleave positive and cleave negative
        if (seqDataArr[-1] == 1):
            cleavePos.append(seqDataArr)
        elif (seqDataArr[-1] == 0):
            cleaveNeg.append(seqDataArr)
      else:
        break

cleavePos = np.asarray(cleavePos)
cleaveNeg = np.asarray(cleaveNeg)

t_jy0 = cleaveNeg.sum(axis = 0)
featureCount = cleavePos.shape[1]
t_jy1 = cleavePos.sum(axis = 0)
n0 = cleaveNeg.shape[0]
n1 = cleavePos.shape[0]
n = n1 + n0
prob_cleaveNeg = n0 / n
prob_cleavePos = n1 / n
k = 5
accuracyValues = []
for alpha in range(11):
  accuracyValues.append(getEstimates(alpha))

# Plot alpha vs. test set accuracy
plt.plot([i for i in range(0, 11)],accuracyValues)
plt.xlabel('Alpha Values')
plt.ylabel('Accuracy Values')
plt.title("Alpha vs. Test Set Accuracy - for 50 rows of Data")
plt.show()

"""DISCUSSION:
When the whole dataset is used MAP estimate decreases accuracy as can be seen from the first plot. However when we only used the first 50 rows of the dataset, we observe a dramatic accuracy decrease observed from the 2nd plot at alpha values between 0 and 1. Additive smoothing improves results by preventing conditional probability to be 0 for some data. When we alter the sample size the prior distribution changes and therefore we see a decrease in performance with respect to alpha values, this case can be seen from the 2nd plot where 50 rows of data were used.

Q3.5 Mutual Information
"""



cleavePos = []
cleaveNeg = []
# load train data
with open('q2_train_set.txt') as f:
    seqData_train = f.readlines()
    for eachSeqData_train in seqData_train:
       seqDataArr = np.fromstring(eachSeqData_train, dtype=int, sep=',')
       N_features = seqDataArr.shape[0] - 1
       # clasify test data as cleave positive and cleave negative
       if (seqDataArr[-1] == 1):
          cleavePos.append(seqDataArr)
       elif (seqDataArr[-1] == 0):
          cleaveNeg.append(seqDataArr)
cleavePos = np.asarray(cleavePos)
cleaveNeg = np.asarray(cleaveNeg)
N_features = cleavePos.shape[1]

# calculating N11: (no of aa that contain cleavage index and is labelled 1), N10, N01, N00 
N_train = cleavePos.shape[0] + cleaveNeg.shape[0]
features_mutualInfo = []
n11 = 0
n10 = 0
n01 = 0
n00 = 0

for j in range(N_features):
  for i in range(cleavePos.shape[0]):
    if (cleavePos[:,j][i] == 1):
      n11 += 1
    if (cleavePos[:,j][i] == 0):
      n01 += 1
  for i in range(cleaveNeg.shape[0]):
    if (cleaveNeg[:,j][i] == 1):
      n10 += 1
    if (cleaveNeg[:,j][i] == 0):
      n10 += 1
  # calculating mutual information
  n = n11 + n10 + n01 + n00
  n_1 = n11 + n01
  n1_ = n10 + n11
  n_0 = n10 + n00
  n0_ = n00 + n01
  if n1_*n_1 == 0 or n0_*n_1 == 0 or n1_*n_0 == 0 or n_0*n0_ == 0:
    mutual_info = float(inf)
  else:
    mutual_info = (n11/n)*np.log2( (n*n11)/(n1_*n_1) ) + (n01/n)*np.log2( (n*n01)/(n0_*n_1) ) 
    + (n10/n)*np.log2( (n*n10)/(n1_*n_0) ) + (n00/n)*np.log2( (n00*n)/(n_0*n0_) )
  features_mutualInfo.append((mutual_info, j))
features_mutualInfo = np.asarray(features_mutualInfo)
features_mutualInfo = features_mutualInfo[features_mutualInfo[:,0].argsort()[::-1]]

testData = []
testData_labelResults = []
# loading test data
with open('q2_test_set.txt') as f:
  seqData_test = f.readlines()
  for eachSeqData_test in seqData_test:
     testDataArr = np.fromstring(eachSeqData_test, dtype=int, sep=',')
     testData.append(testDataArr)
     testData_labelResults.append(testDataArr[-1])
testData = np.asarray(testData)
testData_labelResults = np.asarray(testData_labelResults)
N_train = testData.shape[0]

def getEstimatesUsingMutualInfo(k):
  
  testData = []
  testData_labelResults = []
  # loading test data
  with open('q2_test_set.txt') as f:
    seqData_test = f.readlines()
    for eachSeqData_test in seqData_test:
 	    testDataArr = np.fromstring(eachSeqData_test, dtype=int, sep=',')
 	    testData.append(testDataArr)
 	    testData_labelResults.append(testDataArr[-1])
  testData = np.asarray(testData)
  testData_labelResults = np.asarray(testData_labelResults)
  N_train = testData.shape[0]

  N_train = testData.shape[0]
  cleavePos_yi_max = np.zeros(N_train)
  cleaveNeg_yi_max = np.zeros(N_train)
  prob_x1_y0 = np.zeros(k)
  prob_x1_y1 = np.zeros(k)
  for j in range(k):
    prob_x1_y0[j] = (np.sum(cleaveNeg, axis=0))[int(features_mutualInfo[j][1])] / (cleaveNeg.shape[0])
    prob_x1_y1[j] = (np.sum(cleavePos, axis=0))[int(features_mutualInfo[j][1])] / (cleavePos.shape[0])

  for i in range(N_train):
    cleavePos_yi = np.zeros( k)
    cleaveNeg_yi = np.zeros( k)
    for j in range(k):
      #print(testData[:,int(features_mutualInfo[j][1])])
      #print(j)
      if (testData[:,int(features_mutualInfo[j][1])][i] == 1):
        cleaveNeg_yi[j] = np.log(prob_x1_y0[j])
        cleavePos_yi[j] = np.log(prob_x1_y1[j])
      else:
        cleaveNeg_yi[j] = np.log(1-prob_xj_y0[j])
        cleavePos_yi[j] = np.log(1-prob_xj_y1[j])
    
    cleavePos_yi_max[i] = (np.log(prob_cleavePos) + np.sum(cleavePos_yi))
    cleaveNeg_yi_max[i] = (np.log(prob_cleaveNeg) + np.sum(cleaveNeg_yi))
  
  predicted_labels = []
  for i in range(N_train):
    if (cleavePos_yi_max[i] > cleaveNeg_yi_max[i]):
      predicted_labels.append(1)
    else: 
      predicted_labels.append(0)
  predicted_labels = np.array(predicted_labels)

  # calculating accuracy
  match_count = 0
  fail_count = 0
  for i in range(N_train):
   if (testData_labelResults[i] == predicted_labels[i]):
      match_count += 1
   else:
     fail_count += 1
  accuracy = 100 * (match_count / (fail_count + match_count))
  return accuracy

accuracies = []
for i in range(1,161):
  accuracies.append(getEstimatesUsingMutualInfo(i))
most_accurate_k = np.where(accuracies == np.amax(accuracies))

print("k value that yielded the most accuracy: ", most_accurate_k[0][0], " " ,accuracies[most_accurate_k[0][0]])

# Accuracy for 3.1 was 94.50757575757575

"""DISCUSSION: k value that yielded the most accuracy was 158 with accuracy 98.106% which is greater than the accuracy value obtained in part 3.1 which was 94.508%.

Q3.6 - PCA
"""

labels = []
features_list = []
# load train data
with open('q2_train_set.txt') as f:
    seqData_train = f.readlines()
    for eachSeqData_train in seqData_train:
        seqDataArr = np.fromstring(eachSeqData_train, dtype=int, sep=',')
        N_features = seqDataArr.shape[0] - 1
        labels.append(seqDataArr[-1])
        features_list.append(seqDataArr[:-1])

labels = np.asarray(labels)
df_features = pd.DataFrame(features_list)

# finding the covariance matrix
feature_vectors = df_features.T
cov_matrix = np.cov(feature_vectors)
cov_matrix
eigen_values, eigen_vectors = np.linalg.eig(cov_matrix) 
np.sort(eigen_values)
np.sort(eigen_vectors)

# using the top 3
eigen_vals_first3  = eigen_values[:3]
eigen_vecs_first3 = eigen_vectors[:,:3]
#print("eigen values: ",eigen_vals_first3)
#print("eigen vectors: ",eigen_vecs_first3)

# variance 
covered_var = eigen_values[:3].sum() / eigen_values.sum()
print("% of variance covered is: ",covered_var)

# projection
projected_fv = df_features.dot(eigen_vectors.T[:,:3])

# Plot of 3D Projections
import cmath 
from mpl_toolkits.mplot3d import Axes3D

fig = plt.figure(figsize=(16,16))
ax = fig.add_subplot(111, projection='3d')
ax = fig.gca(projection='3d')

pc1 = projected_fv[0].real
pc2 = projected_fv[1].real
pc3 = projected_fv[2].real
pc1.tolist()
pc2.tolist()
pc3.tolist()

data = (pc1, pc2, pc3)
colors = ('r', 'g', 'b')
names = ('PC1', 'PC2','PC3')

ax.scatter(pc1, -pc2, -pc3, c='r', label='PC1')
ax.scatter(-pc1, pc2, -pc3, c='g', label='PC2')
ax.scatter(-pc1, -pc2, pc3, c='b', label='PC3')

plt.title('Plot of 3D Projections')
plt.legend(loc=2)
ax.set_xlabel('x-axis')
ax.set_ylabel('y-axis')
ax.set_zlabel('z-axis')
plt.show()

print(np.amax(pc1))
print(np.amax(pc2))
print(np.amax(pc3))

"""DISCUSSION:
% of variance covered is: 0.06980266880746583. which is 6% of variance for original dataset. From the PC data the variances go up to 84%, it can be seen that variance is maximazed for all 3 principle components. However, from the 3D Plot of projected data, I conclude that it is not feasible to use PCA regarding that the data is still not easily seperable.
"""